{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e278bc0-c1e4-4d21-bcd0-1d92c794f61e",
   "metadata": {},
   "source": [
    "## Q1. What is the purpose of forward propagation in a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315236d5-b970-42b1-838f-391ee663545d",
   "metadata": {},
   "source": [
    "-> Forward propagation in neural network is a process of passing inpute data through the network to optain the output.\n",
    "\n",
    "-> The primary purpose of the forward propagation is to get the output based on input data and current state of weights and bias.\n",
    "\n",
    "-> Back propagation is crucial because it determines that how well current state of model will perform in terms of making prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf668c24-8a63-4c47-8451-68598065c505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf5c3c0a-4c02-4584-a246-6c40eeefa148",
   "metadata": {},
   "source": [
    "## Q2. How is forward propagation implemented mathematically in a single-layer feedforward neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a870105-2ba6-43d8-b0b6-8dc4f74c5d02",
   "metadata": {},
   "source": [
    "* Steps\n",
    "\n",
    "Step - 1 **Inputes**             : The network recieves the input vector as X size of n.\n",
    " \n",
    "Step - 2 **Weights**             : Each input feature is associated with the weights Wi.\n",
    "\n",
    "Step - 3 **Bias**                : Bias will be added on the weighted sum of input for handle the output independently of the input.\n",
    "\n",
    "Step - 4 **Activation Function** : Activation function will be applied to the value which we got after adding the bias.It introduces the non-linearity to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9058f-9286-4e2e-8e5f-28f4b0bb536d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f837a79c-6062-41ae-a8da-efd1d775b4b3",
   "metadata": {},
   "source": [
    "## Q3. How are activation functions used during forward propagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e688034-35f2-46da-9a09-bdd3fe1e2f4e",
   "metadata": {},
   "source": [
    "* **Non-linearity Introduction** : It intoduces the non-linearity into the network which help to learn the complex relationship.\n",
    "\n",
    "* **Application of Activation Function** : Activation functions are applied to the weighted sum of input. The purpose of the activation function is to transform the weighted sum into out for next layer or to consider it as a final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6266d471-d771-4df6-b64d-170a64b56c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14ef8508-7539-4a4d-b3ec-a572f61d73e4",
   "metadata": {},
   "source": [
    "## Q4. What is the role of weights and biases in forward propagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8dd62-cdd7-41b2-9110-b4fd05c22dae",
   "metadata": {},
   "source": [
    "Wights and Biases are the parameters that network learnes and optimizes during the training process.\n",
    "\n",
    "* **Weights**\n",
    "\n",
    "**Scaling Inputs** : Weights are used to scale the input data. It determines the importance of every input data into decision-making process of the network.\n",
    "\n",
    "**Weighted Sum** : Input data are multiplied by the weights and then summed up. After summing them we will add bias which make the value weighted some.\n",
    " \n",
    "**Z** = sum(XiWi) + B **,** where X=Input Data, W= Weights, B= Bias\n",
    "              \n",
    "**Learning Parameter** : Weights are the primary parameter that netowrk adjusts during the training by backpropagation. By modifying weights network learns to make more accurate prediction prediction. Proper weight adjustment allows the network to make more complex relationship in the data.\n",
    "\n",
    "* **Biases**\n",
    "\n",
    "**Shifting Activation Fumction** : Bias allows the activation function to be shifted left or right. It helps the network to fit the data well by providing more flexibility into the decision boundry.\n",
    "\n",
    "**Handling Zero Inputs** : Bias allows the neuron to fire even when the inputs are zero.\n",
    "\n",
    "**Learning from Data** : Bias is also a learnable parameter like weights, it is adjusted by the backpropagation during the training process to achieve the better outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e698182-e324-4517-8cf8-0214b5005bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "595e3ccf-dc88-41a4-be5f-2119fb815132",
   "metadata": {},
   "source": [
    "## Q5. What is the purpose of applying a softmax function in the output layer during forward propagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6b16a3-db9b-447f-b359-a07daf865466",
   "metadata": {},
   "source": [
    "**Probability Distribution** : Softmax comverts the logits into the probability distribution. Every value can have probability ranges between 0,1 and the sum of all probabilities will be 1.\n",
    "\n",
    "**Class Prediction** : The class with the highest probability will be taken as the predicted class, which can help in decision-making for classification problem.\n",
    "\n",
    "**Normalization** : Softmax ansures the input is normalized, especially for the large magnitude value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9934c-4086-42b6-b390-8efa31866dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90dfbb32-2c81-4f43-b2a8-06a3541b58d3",
   "metadata": {},
   "source": [
    "## Q6. What is the purpose of backward propagation in a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070f8d57-2d0f-4a6f-a981-75e99a7e4166",
   "metadata": {},
   "source": [
    "**Error Minimization** : Backpropagation calculates the gradient of the loss function with respect to each weight and bias in the network, enabling the optimization process to adjust these parameters in a way that reduces the error.\n",
    "\n",
    "**Gradient Computation** : Bachpropagation calculates the gradient of loss with respect to weights and bias by using the chain rule of the calculus. The gradient denotes how a small change in weights and bias will affect the loss, guiding the optimization algorithm to update the parameters.\n",
    "\n",
    "**Parameter Update** : Optimization algorithm updates the weights and biases by using the gradient. Parameters are adjusted to opposite direction of the gradient to fine the global minema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea628d9e-a5dd-4eb5-8ad0-aa93574d902c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed2b6828-cf69-41e6-8c30-f3f7095eb7f3",
   "metadata": {},
   "source": [
    "## Q7. How is backward propagation mathematically calculated in a single-layer feedforward neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb173496-8842-4e63-a5ce-f0d2a051abe0",
   "metadata": {},
   "source": [
    "Back Propagation involves calculating the gradients of the loss function with respect to the weights and biases, and then updating these parameters to minimize the loss.\n",
    "\n",
    "* Computes the gradient of loss with respect to weights and bias :\n",
    "\n",
    "Weights : ∂w/∂L= ∂z/∂L * ∂w/∂z = ∂z/∂L * x\n",
    "\n",
    "Bias : ∂b/∂L= ∂z/∂L * ∂b/∂z = ∂z/∂L * 1 = ∂L/∂z\n",
    "\n",
    "* **Parameter Update**\n",
    "\n",
    "Weight Updation : w = w - η * ∂w/∂L\n",
    "\n",
    "Bias Updation   : b = b - η * ∂b/∂L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4b9602-4d93-4ff1-b920-8a2e291a0cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "732702c4-490c-4101-889d-74534caec874",
   "metadata": {},
   "source": [
    "## Q8. Can you explain the concept of the chain rule and its application in backward propagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155f7c8e-fed4-460e-b6b4-b5f5527fa83b",
   "metadata": {},
   "source": [
    "**Concept of Chain Rule**\n",
    "\n",
    "The chain rule is a fundamental concept in calculus used to compute the derivative of a composite function. \n",
    "\n",
    "The chain rule states that, if a variable Y depends on U and variable U depends on X so the derivative of Y with respect to X will be : dY/dX = dY/dU * dU/dX .\n",
    "\n",
    "**Application in Back Propagation**\n",
    "\n",
    "In the context of backward propagation in neural networks, the chain rule allows us to compute the gradient of the loss function with respect to each weight and bias by breaking down the complex computation into simpler, manageable parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b364fcf-6315-43e1-ab6f-e5f58d9ff676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0624d602-0ed6-4ec2-a527-7330f8f65b19",
   "metadata": {},
   "source": [
    "## Q9. What are some common challenges or issues that can occur during backward propagation, and how can they be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc57a8-3bcf-44b7-a0e8-0a2dc7a0527e",
   "metadata": {},
   "source": [
    "**Vanishing Gradient**\n",
    "\n",
    "* Issue : During the back propagation, gradient becomes very small due to it passes through maultiple layers. Because of its small value updation process becomes so slowly.\n",
    "\n",
    "* Solution : \n",
    "\n",
    "(i) We can use the activation functions like ReLU or Leaky ReLU.\n",
    "\n",
    "(ii) Batch Normalization\n",
    "\n",
    "(iii) Weights Initialization\n",
    "\n",
    "**Exploiding Gradient**\n",
    "\n",
    "* Issue : Gradient can become too large during back propagation, it can become cause of large weight updation which make the process unstable.\n",
    "\n",
    "* Solution : \n",
    "\n",
    "(i) We can limit the gradient value to some threshold, which can't let the value of gradient become large.\n",
    "\n",
    "(ii) Weights Initialization\n",
    "\n",
    "(iii) Can use the better optimizers like ADAM and RMS Prop.\n",
    "\n",
    "**Overfitting**\n",
    "\n",
    "* Issue : The network performs well on the training data but poorly on unseen data due to memorizing the training data rather than learning general patterns.\n",
    "\n",
    "* Solution : \n",
    "\n",
    "(i) We can use the regularization techniques like L1 and L2 Regularization.\n",
    "\n",
    "(ii) Monitor the performance on a validation set and stop training when performance stops improving. \n",
    "\n",
    "(iii) Randomly dropping units during training helps prevent overfitting by making the network less reliant on any particular neuron.\n",
    "\n",
    "**Internal Covariate Shift**\n",
    "\n",
    "* Issue : The distribution of each layer's input changes during training, so it can slow down the training process.\n",
    "\n",
    "* Solution : We can mitigate this problem by batch normalization, which normalizes the input data and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8574924-729c-4862-859d-e420146087ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
